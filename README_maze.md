# ðŸ§  MazeRL: Custom Maze Environment for Reinforcement Learning

This project implements a customizable maze environment for reinforcement learning experiments.  
Agents can explore, observe partial environments, and learn to reach random goals using tabular or heuristic policies.

---

## ðŸš€ Features

- âœ… Procedurally generated mazes (DFS, Prim, Growing Tree)
- âœ… Supports multi-goal, traps, non-deterministic moves
- âœ… Fully and partially observable variants
- âœ… Directional agents with limited field of view
- âœ… CLI-controlled map size and random seeds
- âœ… Output trajectory, view, facing, actions, success logs

---

## ðŸ“‚ Folder Structure

```
maze/
â”œâ”€â”€ env/           # Custom Gym-style environments
â”œâ”€â”€ train/         # Exploration and trajectory generation
â”œâ”€â”€ run/           # Run trained policies or visualize agent paths
â”œâ”€â”€ outputs/       # Saved .npy, .jsonl, .json for training
â”œâ”€â”€ visual/        # Rendering utilities
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ðŸ’» Quick Start

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Run exploration script

```bash
python train/train_real_auto.py --seed 101 --size 15
```

> Outputs will be saved in the `outputs/` directory.

---

## ðŸ”® Future Integration

This environment supports data generation for GPT-based fine-tuning (e.g., predicting goals or optimal actions from partial observations).

---

## ðŸ‘¤ Author

Developed by [@Seanaaa0](https://github.com/Seanaaa0)  
Focus: Reinforcement Learning, LLM data generation, intelligent agent systems.
